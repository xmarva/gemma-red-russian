{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":10336107,"sourceType":"datasetVersion","datasetId":6284670},{"sourceId":26140,"sourceType":"modelInstanceVersion","modelInstanceId":22003,"modelId":3301}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>⚠️ ToDo:</b> <br>\n\n<div>\n    <input type=\"checkbox\" id=\"scales\" name=\"scales\" />\n    <label for=\"scales\">Restructure and upload full dataset</label>\n</div>\n\n<div>\n    <input type=\"checkbox\" id=\"scales\" name=\"scales\" />\n    <label for=\"scales\">Add wandb parameter logging and visualization</label>\n</div>\n\n<div>\n    <input type=\"checkbox\" id=\"scales\" name=\"scales\" />\n    <label for=\"scales\">Train model on full dataset</label>\n</div>\n\n<div>\n    <input type=\"checkbox\" id=\"scales\" name=\"scales\" />\n    <label for=\"scales\">Add markdown cells with comments and explanation</label>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes peft trl wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:28:19.040037Z","iopub.execute_input":"2024-12-30T18:28:19.040440Z","iopub.status.idle":"2024-12-30T18:28:41.973615Z","shell.execute_reply.started":"2024-12-30T18:28:19.040400Z","shell.execute_reply":"2024-12-30T18:28:41.972345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport os\nimport json\nimport torch\nfrom tqdm import tqdm\nfrom datasets import Dataset\nfrom collections import defaultdict\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer\n)\n\nfrom peft import (\n    prepare_model_for_kbit_training,\n    LoraConfig,\n    get_peft_model\n)\n\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:28:41.976313Z","iopub.execute_input":"2024-12-30T18:28:41.977052Z","iopub.status.idle":"2024-12-30T18:29:02.443468Z","shell.execute_reply.started":"2024-12-30T18:28:41.977006Z","shell.execute_reply":"2024-12-30T18:29:02.442828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Authentication","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF\")\nwandb_token = user_secrets.get_secret(\"wandb\")\n\nlogin(hf_token)\nwandb.login(key=wandb_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:02.444309Z","iopub.execute_input":"2024-12-30T18:29:02.444533Z","iopub.status.idle":"2024-12-30T18:29:04.450085Z","shell.execute_reply.started":"2024-12-30T18:29:02.444510Z","shell.execute_reply":"2024-12-30T18:29:04.449116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config & Hyperparameters","metadata":{}},{"cell_type":"code","source":"SEED = 42\nMODEL_NAME = \"google/gemma-2b-it\"\n\nDATASET_PATH = \"/kaggle/input/red-russian/data/red_russian_dataset.json\"\nAUTHORS = [\"Stalin\"]\nNUM_BOOKS = 1\n\nMAX_LENGTH = 256\nBATCH_SIZE = 6\nGRADIENT_ACCUMULATION_STEPS = 2\nLEARNING_RATE = 5e-4\nEPOCHS = 1\nLOGGING_STEPS = 10\n\nSAVE_STRATEGY = \"epoch\"\nOUTPUT_DIR = \"./red_russian_gemma\"\nWANDB_PROJECT = \"red_russian_gemma\" \nWANDB_RUN_NAME = \"red_russian_gemma-run-1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.451259Z","iopub.execute_input":"2024-12-30T18:29:04.452034Z","iopub.status.idle":"2024-12-30T18:29:04.456976Z","shell.execute_reply.started":"2024-12-30T18:29:04.451991Z","shell.execute_reply":"2024-12-30T18:29:04.456060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"def load_json_data(dataset_path):\n    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n        json_data = json.load(f)\n    if not isinstance(json_data, list):\n        raise ValueError(\"JSON data must be a list of dictionaries\")\n        \n    return json_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.458118Z","iopub.execute_input":"2024-12-30T18:29:04.458473Z","iopub.status.idle":"2024-12-30T18:29:04.470843Z","shell.execute_reply.started":"2024-12-30T18:29:04.458432Z","shell.execute_reply":"2024-12-30T18:29:04.470075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_and_limit_data(json_data, authors=None, num_books=None):\n    \n    filtered_list = []\n    author_counts = defaultdict(int) \n\n    for item in json_data:\n        author = item[\"author_name\"]\n\n        if authors is None or author in authors:\n            if num_books is None or author_counts[author] < num_books:\n                filtered_list.append(item)\n                if num_books is not None:\n                    author_counts[author] += 1\n\n        n = len(filtered_list)\n        print(f\"Data successfully filtered by author and quantity, loaded {n} books\")\n\n    return filtered_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.472705Z","iopub.execute_input":"2024-12-30T18:29:04.472995Z","iopub.status.idle":"2024-12-30T18:29:04.482829Z","shell.execute_reply.started":"2024-12-30T18:29:04.472969Z","shell.execute_reply":"2024-12-30T18:29:04.481979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chunking","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n\n    text = re.sub(r\"^\\d+$\", \"\", text, flags=re.MULTILINE) \n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"^\\s*$\", \"\", text, flags=re.MULTILINE) \n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.483797Z","iopub.execute_input":"2024-12-30T18:29:04.484161Z","iopub.status.idle":"2024-12-30T18:29:04.493107Z","shell.execute_reply.started":"2024-12-30T18:29:04.484120Z","shell.execute_reply":"2024-12-30T18:29:04.492331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_data(json_data, chunk_size=256, use_authors=None):\n    \n    chunks = []\n\n    for item in json_data: \n        if 'book_content' not in item:\n            print(f\"Warning: Skipping item due to missing 'book_content': {item}\")\n            continue\n\n        text = item['book_content']\n        \n        if use_authors and 'author_name' in item:\n            author = item['author_name']\n        else:\n            author = \"Unknown\"\n\n        text = clean_text(text)\n\n        for i in range(0, len(text), chunk_size):\n            chunk = text[i:i + chunk_size]\n            author_token = f\"<|author:{author}|>\"\n            chunks.append(f\"{author_token} {chunk}\")\n\n    if not chunks:\n        print(\"Error: No valid text data found in the JSON file.\")\n\n        return None\n\n    print(\"Data successfully chunked\")\n\n    dataset = Dataset.from_dict({\"text\": chunks})\n    print(f\"Dataset loaded: {len(dataset)} samples\")\n    \n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.494114Z","iopub.execute_input":"2024-12-30T18:29:04.494433Z","iopub.status.idle":"2024-12-30T18:29:04.510171Z","shell.execute_reply.started":"2024-12-30T18:29:04.494396Z","shell.execute_reply":"2024-12-30T18:29:04.509386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"def prepare_tokenizer(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        padding_side='right'\n    )\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    return tokenizer\n\ndef tokenize_function(examples, tokenizer, max_length):\n    tokenized_inputs = tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\n    # Shift the input ids to the right\n    labels = tokenized_inputs[\"input_ids\"].clone()\n    labels[:, :-1] = labels[:, 1:]\n    # Replace padding with ignore index\n    labels[:, -1] = -100\n\n    return {\n        \"input_ids\": tokenized_inputs[\"input_ids\"],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n        \"labels\": labels\n    }\n\ndef prepare_training_data(dataset, tokenizer, max_length):\n    tokenized_dataset = dataset.map(\n        lambda examples: tokenize_function(examples, tokenizer, max_length),\n        batched=True,\n        remove_columns=dataset.column_names\n    )\n\n    print(\"Data successfully tokenized\")\n    \n    return tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.511070Z","iopub.execute_input":"2024-12-30T18:29:04.511391Z","iopub.status.idle":"2024-12-30T18:29:04.520694Z","shell.execute_reply.started":"2024-12-30T18:29:04.511351Z","shell.execute_reply":"2024-12-30T18:29:04.519878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def configure_model(model_name):\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        load_in_4bit=True\n    )\n    model = prepare_model_for_kbit_training(model)\n\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=32,\n        target_modules=[\n            \"q_proj\", \"o_proj\",\n            \"k_proj\", \"v_proj\",\n            \"gate_proj\", \"up_proj\"\n        ],\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.521607Z","iopub.execute_input":"2024-12-30T18:29:04.521890Z","iopub.status.idle":"2024-12-30T18:29:04.536337Z","shell.execute_reply.started":"2024-12-30T18:29:04.521863Z","shell.execute_reply":"2024-12-30T18:29:04.535496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, num_items_in_batch=2, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.537339Z","iopub.execute_input":"2024-12-30T18:29:04.537615Z","iopub.status.idle":"2024-12-30T18:29:04.547309Z","shell.execute_reply.started":"2024-12-30T18:29:04.537591Z","shell.execute_reply":"2024-12-30T18:29:04.546558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, tokenizer, tokenized_dataset, output_dir, batch_size, gradient_accumulation_steps, learning_rate, epochs, logging_steps, save_strategy, wandb_project, wandb_run_name, seed):\n\n    torch.manual_seed(seed)\n    wandb.init(project=wandb_project, name=wandb_run_name)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        learning_rate=learning_rate,\n        logging_dir=\"./logs\",\n        logging_steps=logging_steps,\n        save_strategy=save_strategy,\n        fp16=False,\n        bf16=torch.cuda.is_available(),\n        gradient_checkpointing=False,\n        report_to=\"wandb\", #Log to W&B\n        seed = seed\n    )\n\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        tokenizer=tokenizer\n    )\n\n    trainer.train()\n\n    trainer.save_model(output_dir)\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.548101Z","iopub.execute_input":"2024-12-30T18:29:04.548309Z","iopub.status.idle":"2024-12-30T18:29:04.559755Z","shell.execute_reply.started":"2024-12-30T18:29:04.548288Z","shell.execute_reply":"2024-12-30T18:29:04.559051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_data = load_json_data(DATASET_PATH)\njson_data = filter_and_limit_data(json_data, AUTHORS, NUM_BOOKS)\ndataset = chunk_data(json_data, MAX_LENGTH, AUTHORS)\n\nif dataset is None:\n    print(\"Dataset loading failed. Aborting.\")\nelse:\n    tokenizer = prepare_tokenizer(MODEL_NAME)\n    tokenized_dataset = prepare_training_data(dataset, tokenizer, MAX_LENGTH)\n\n    model = configure_model(MODEL_NAME)\n\n    train_model(model, tokenizer, tokenized_dataset, OUTPUT_DIR, BATCH_SIZE, GRADIENT_ACCUMULATION_STEPS,\n               LEARNING_RATE, EPOCHS, LOGGING_STEPS, SAVE_STRATEGY, WANDB_PROJECT, WANDB_RUN_NAME, SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T18:29:04.560739Z","iopub.execute_input":"2024-12-30T18:29:04.561347Z","execution_failed":"2024-12-30T18:33:13.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"def generate_text(model, tokenizer, prompt, max_length=100):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        num_return_sequences=1,\n        return_full_text=False,\n        temperature=0.7\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-30T18:33:13.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- Inference Examples ---\")\n\nprompts = [\n    \"Мы пойдём другим путём?\",\n    \"Каждая кухарка должна научиться управлять государством\",\n    \"### Stalin: \\nРасскажи мне о Ленине\",\n    \"### Lenin:\\nКакова цель революции?\"\n]\n\nfor prompt in prompts:\n    generated_text = generate_text(model, tokenizer, prompt)\n    print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-30T18:33:13.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [\n    \"Мы пойдём другим путём?\",\n    \"Каждая кухарка должна научиться управлять государством\",\n    \"<|author:Stalin|> Расскажи мне о Ленине\", \n    \"<|author:Lenin|> Какова цель революции?\" \n]\n\nfor prompt in prompts:\n    generated_text = generate_text(model, tokenizer, prompt)\n\n    match = re.match(r\"<\\|author:(.*?)\\|> \", generated_text)\n    if match:\n        author = match.group(1)\n        generated_text = generated_text[match.end():].strip()\n        print(f\"Generated Author: {author}\")\n\n    print(f\"Prompt: {prompt}\")\n    print(f\"Generated Text:\\n{generated_text}\\n---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}